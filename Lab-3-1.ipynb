{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexwolson/carte-biozone-workshop/blob/main/Lab-3-1.ipynb)\n",
    "\n",
    "# CARTE-BioZone Workshop on Machine Learning\n",
    "#### Wednesday, August 30, 2023\n",
    "#### Lab 1, Day 3: Neural Networks\n",
    "##### Lab author: Alex Olson\n",
    "\n",
    "#### Introduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWWYgttBIhuX"
   },
   "source": [
    "In this lab, we will be taking our first look at developing our own *neural networks* (NN) with [PyTorch](https://pytorch.org/), probably the most popular machine learning library for working with NNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Check if we are running on Google Colab, or locally\n",
    "import sys\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T15:54:30.622265Z",
     "start_time": "2023-08-24T15:54:30.618460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    !pip install -q torch torchvision torchaudio numpy pandas matplotlib scikit-learn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T15:55:02.404429Z",
     "start_time": "2023-08-24T15:54:59.958110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LWQyyScGIhuY",
    "outputId": "c89e6259-2580-4fda-e213-3751a1095dcb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-08-24T15:56:45.557048Z",
     "start_time": "2023-08-24T15:56:45.019664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Data Manipulation Libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-Learn Libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Cj0g_bBIhuZ"
   },
   "source": [
    "### An Intuitive Intro to Neural Nets\n",
    "\n",
    "In today's lab, we're going to start with an intuitive exercise on the Titanic dataset using Logistic Regression and a simple Neural Network before moving onto some more complex stuff. Let's start by loading our dataset, and cleaning it up as we have done before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YYb9noojIhua",
    "outputId": "c4db7dc3-0ca8-4b97-e62e-7daa906ecf6f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "ExecuteTime": {
     "end_time": "2023-08-24T15:57:49.001647Z",
     "start_time": "2023-08-24T15:57:48.952012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   pclass  survived  sex      age  sibsp  parch      fare\n0       1         1    0  29.0000      0      0  211.3375\n1       1         1    1   0.9167      1      2  151.5500\n2       1         0    0   2.0000      1      2  151.5500\n3       1         0    1  30.0000      1      2  151.5500\n4       1         0    0  25.0000      1      2  151.5500",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pclass</th>\n      <th>survived</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>29.0000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>211.3375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.9167</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_openml(\"titanic\", version=1, as_frame=True, parser=\"auto\").frame\n",
    "data.survived = pd.to_numeric(data[\"survived\"])\n",
    "data.drop([\"boat\", \"body\", \"home.dest\"], axis=1, inplace=True)\n",
    "data = data.drop([\"name\", \"ticket\", \"cabin\", \"embarked\"], axis=1)\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(data[\"sex\"])\n",
    "data[\"sex\"] = label_encoder.transform(data[\"sex\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdHK09KWIhub"
   },
   "source": [
    "Next, as we have become accustomed to doing, we will split the dataset into a training set (where we will do our cross validation) and a test set (our hold-out data). We've done this a few times during the labs, so hopefully you're getting used to the process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CHYYoq9uIhuc",
    "ExecuteTime": {
     "end_time": "2023-08-24T15:57:56.520607Z",
     "start_time": "2023-08-24T15:57:56.501043Z"
    }
   },
   "outputs": [],
   "source": [
    "target_data = data[\"survived\"]\n",
    "feature_data = data.iloc[:, data.columns != \"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_data, target_data, test_size=0.3, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UvxOkuoIhud"
   },
   "source": [
    "Now, we're ready to try out some models on our training data (you haven't seen anything new yet!). Since we're solving a binary classification problem (i.e., predicting a 0 or 1 target), we want to design classifiers. In this exercise, we're going to fit a logistic regression to our data and then design a neural network architecture that behaves exactly like a logistic regression and validate that we get the same result.\n",
    "\n",
    "Logistic regression models are linear models similar to linear regression models. Hopefully you somewhat remember them from lecture. Let's review them, starting with the linear regression equation:\n",
    "\n",
    "\n",
    "<center>$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n$</center>\n",
    "\n",
    "Where $\\hat{y}$ is our prediction, $\\beta$ is our vector of coefficients (the things we learn), and $x$ is our feature vector. The linear regression equation defines a line in $n$ dimensional space. The problem with linear regression is that it doesn't really perform well on classification tasks. Consider the following example:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/linear-classification.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "The green line represents our trained linear regression model. Our feature is the size of a tumor, and our target is whether it is malignant or not (0 or 1). As we can see, even though our model is trained to the data to minimize error, for a lot of the values of tumor size it is going to give us a weird result (e.g., for some really small tumors, the prediction would be a negative value!).\n",
    "\n",
    "To resolve this, we use the *logistic function* (also called the *sigmoid* function) to 'squish' our linear model to be bounded by 0 and 1. The logistic (sigmoid) function is $\\frac{1}{1+e^{-x}}$, and thus our logistic regression equation becomes:\n",
    "\n",
    "<center>$\\large\\hat{y} = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n)}}$</center>\n",
    "\n",
    "In this equation, the values for $\\hat{y}$ can never be below 0, and can never exceed 1 even for the most extreme feature values $x$.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* Assuming you've trained a nice logistic regression model to the below data (see Figure), what might the model fit look like (i.e., what will the line look like)? ____________________________________\n",
    "* For new data samples with features $x$, how would you convert the output of the logistic regression, $\\hat{y}$, into a classification (0 or 1)? ______________________________\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/logistic-classification.jpg?raw=1\" width=\"400\"/>\n",
    "\n",
    "OK, cool! So a quick review of logistic regression. Let's use scikit-learn to fit a logistic regression model to our training set and then predict on our test set (we won't do cross validation this time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t7iInLxCIhue",
    "outputId": "bea2e9ca-a2f7-4bd6-ed84-497adeff6b03",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-08-24T15:59:33.315014Z",
     "start_time": "2023-08-24T15:59:33.263213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy: 79.13%\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "imputer.fit(X_train)\n",
    "\n",
    "X_train = imputer.transform(X_train)\n",
    "\n",
    "logistic_model = LogisticRegression()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "X_test = imputer.transform(X_test)\n",
    "predictions = logistic_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Logistic accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX9iX-X4Ihuf"
   },
   "source": [
    "**YOUR TURN:**\n",
    "* The default regularization parameter for sklearn's logistic regression is L2 (or ridge regression); can you figure out how to change it to L1 (LASSO)? _Note: you'll have to look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)!_ ______________________\n",
    "* What is the mean accuracy of an L1 regularized Logistic regression model on the training set? ______________________\n",
    "\n",
    "OK - time for the good stuff!\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "A *neural network* (NN) is a type of machine learning model that, like linear or logistic regression, takes a feature vector, X, as input and predicts a target, y. The way it does this is a little bit different, however. A typical NN architecture consists of: an input layer, hidden layers, and an output layer. Each layer consists of a set of nodes (neurons) connected by edges (outputs). Let's look at the figure below:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/nn.jpeg?raw=1\" width=\"400\"/>\n",
    "\n",
    "**Input layer**: This is a passive layer that simply takes in your feature data and outputs it to the hidden layers. You can think of each input layer neuron as being associated with a feature in your feature set.\n",
    "\n",
    "**Hidden layer**: This is where the magic happens. The original features, as received by the input layer, go through a series of transformations within the hidden layer. You can think of each node (neuron) within the hidden layer as a highly transformed feature.\n",
    "\n",
    "**Output layer**: This is where we get our final result, the 0 or 1 prediction.\n",
    "\n",
    "### Zooming In\n",
    "\n",
    "Let's take a look at what is happening at any given node (neuron) within the hidden layer. Take a look at the following image of a neuron within an NN:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/neuron.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "Every neuron has some inputs ($x_1, x_2, \\dots, x_n$) with input weights ($w_1, w_2, \\dots, w_n$) and an output, $Y$. The neuron itself applies a transformation, $f$, known as the *activation function*, to the linear combination of its inputs and input weights. The value $b$ is a constant weight called the bias.\n",
    "\n",
    "There are many different types of activation functions, but the popular ones are the *sigmoid*, *tanh*, and *ReLU* activation functions. Yes, you heard correctly: the sigmoid function is a popular activation function! (This should be reminding you of the logistic regression model we discussed above).\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If you were to develop a simple neural network architecture that was equivalent to a logistic regression model for the Titanic data, how would you do it? Get a pen and paper and draw it out. Make sure to specify: the input layer, the hidden layer(s), the output layer, the activation function(s), the weights, and the biases.\n",
    "* How many hidden layers does your NN have? What type of activation function, $f$, does it use? _________________\n",
    "* Say you wanted to add another layer to your NN architecture with 3x neurons, what would your new architecture look like? _________________\n",
    "\n",
    "### Intro to  PyTorch\n",
    "\n",
    "OK, so now that we've made the connection between NNs and Logistic Regression, let's code up our little NN in PyTorch and use it to predict survivorship on the Titanic dataset.\n",
    "\n",
    "First, *tensors* are the fundamental data type of PyTorch. Each tensor is effectively a multi-dimensional array, just like a numpy array. The primary difference is that tensors have been setup in such a way to enhance the NN training process.\n",
    "\n",
    "Let's load our X and y training data into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gxobcMqFIhug",
    "ExecuteTime": {
     "end_time": "2023-08-24T16:20:59.502802Z",
     "start_time": "2023-08-24T16:20:56.954309Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train.values).float()\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_test_tensor = torch.from_numpy(y_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "X_train_tensor.shape, y_train_tensor.shape, X_test_tensor.shape, y_test_tensor.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ge4UKjvvuNy3",
    "outputId": "ac0439a4-04e5-4501-a35f-708a54ebfb0e",
    "ExecuteTime": {
     "end_time": "2023-08-24T16:21:01.785974Z",
     "start_time": "2023-08-24T16:21:01.766012Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([916, 6]),\n torch.Size([916]),\n torch.Size([393, 6]),\n torch.Size([393]))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCDsVueiIhuh"
   },
   "source": [
    "Next, we will actually define our logistic regression network model class. The below function, `LogisticRegression`, applies a sigmoid transformation to the output, as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vnbPW--OIhuh",
    "ExecuteTime": {
     "end_time": "2023-08-24T16:22:18.807149Z",
     "start_time": "2023-08-24T16:22:18.787683Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBNTYMbRIhuh"
   },
   "source": [
    "Next, we identify the dimensions of our problem: 6 x 1 (6 features and 2 target classes: 0 or 1), initialize our model with those dimensions and then specify the loss function ([cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)) and optimization technique ([stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zAfHj63fIhui",
    "ExecuteTime": {
     "end_time": "2023-08-24T16:22:19.597796Z",
     "start_time": "2023-08-24T16:22:19.585917Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 6\n",
    "output_dim = 1\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T16:22:20.339839Z",
     "start_time": "2023-08-24T16:22:20.330827Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyFpvFy_Ihui"
   },
   "source": [
    "Next, we set up our dataset to be *iterable* such that we can train our neural network in *batches*. A batch is a subset of the total data such that if we combined them all, we'd get the whole dataset. Batching is done to speed up the training process and reduce memory requirements.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If we select a batch size of 256, how many batches of training data will be generated?________________\n",
    "\n",
    "The `DataLoader` function does this batching operation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yTtyV_mVIhuj",
    "ExecuteTime": {
     "end_time": "2023-08-24T16:22:57.119072Z",
     "start_time": "2023-08-24T16:22:57.096294Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ltyMFvsIhuj"
   },
   "source": [
    "Next, we train our NN and test its performance on the test set every 4000 iterations. We use 40,000 *epochs* and print our accuracy values every 4000 iterations. An epoch is when the entire dataset has passed through the network. An iteration is when a single forward/backward pass of the network over a batch of data is done.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If our batch size is 256 and we elect to do 40,000 epochs (i.e., the network sees the entire dataset 40,000 times), how many iterations (forward/backward passes of the data) will our network see? _______________________\n",
    "\n",
    "Run the below code and check out the incremental accuracy improvement output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "q2lxP28lIhuj",
    "outputId": "fc7f5f42-5779-47cd-e856-8051a103ad93",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-08-24T16:33:39.660202Z",
     "start_time": "2023-08-24T16:29:49.437043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/40000 | Loss: 0.45 | Accuracy: 0.79 | Progress: 0%\n",
      "Epoch: 4000/40000 | Loss: 0.43 | Accuracy: 0.79 | Progress: 10%\n",
      "Epoch: 8000/40000 | Loss: 0.45 | Accuracy: 0.79 | Progress: 20%\n",
      "Epoch: 12000/40000 | Loss: 0.50 | Accuracy: 0.79 | Progress: 30%\n",
      "Epoch: 16000/40000 | Loss: 0.46 | Accuracy: 0.79 | Progress: 40%\n",
      "Epoch: 20000/40000 | Loss: 0.45 | Accuracy: 0.80 | Progress: 50%\n",
      "Epoch: 24000/40000 | Loss: 0.41 | Accuracy: 0.79 | Progress: 60%\n",
      "Epoch: 28000/40000 | Loss: 0.49 | Accuracy: 0.79 | Progress: 70%\n",
      "Epoch: 32000/40000 | Loss: 0.45 | Accuracy: 0.80 | Progress: 80%\n",
      "Epoch: 36000/40000 | Loss: 0.44 | Accuracy: 0.79 | Progress: 90%\n"
     ]
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Training parameters\n",
    "total_epochs = 40000\n",
    "log_interval = 4000  # Interval for logging progress\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(total_epochs):\n",
    "    for batch_idx, (batch_features, batch_target) in enumerate(train_loader):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        predictions = model(batch_features)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(torch.squeeze(predictions), batch_target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Logging progress\n",
    "    if epoch % log_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_predictions = model(X_test_tensor)\n",
    "            test_predicted_classes = torch.round(test_predictions.data).numpy()\n",
    "\n",
    "        model.train()\n",
    "        test_accuracy = accuracy_score(y_test, test_predicted_classes)\n",
    "        print(\n",
    "            f\"Epoch: {epoch}/{total_epochs} | \"\n",
    "            f\"Loss: {loss.item():.2f} | \"\n",
    "            f\"Accuracy: {test_accuracy:.2f} | \"\n",
    "            f\"Progress: {epoch / total_epochs * 100:.0f}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuXmjwuMIhuk"
   },
   "source": [
    "Hopefully you got an accuracy of around ~78%. What you'll notice is that is similar accuracy we got from sklearn's built-in logistic regression function from earlier in the lab. There are differences in the implementation that account for this discrepancy, notably sklearn used l2 regularization, and an optimizer called LBFGS instead of SGD that we learned and used.\n",
    "\n",
    "Let's take a look at the trained model parameters using the `model.parameters()` function within PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "535umxhZIhuk",
    "outputId": "293256b3-aeb4-4a72-95f2-902e62245d80",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2023-08-24T16:33:58.281564Z",
     "start_time": "2023-08-24T16:33:58.261426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[Parameter containing:\n tensor([[-0.7323, -2.4649, -0.0197, -0.3609, -0.0120,  0.0048]],\n        requires_grad=True),\n Parameter containing:\n tensor([3.2712], requires_grad=True)]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(model.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkfrWNPGIhuk"
   },
   "source": [
    "Hm, well this is interesting! We can see that our model consists of two tensors: the first has (1,6), and the second has dimension (1). Refer back to how you drew what you thought this NN architecture would look like.\n",
    "\n",
    "**YOU TURN:**\n",
    "* What do you think these values represent? ____________________________\n",
    "* How many hidden layers does the architecture have? ______________________________\n",
    "* Draw the architecture and label (some of) the weights (trained parameters). ______________________________\n",
    "\n",
    "(*Hint: to answer these questions, try printing `outputs.data[0]` and `predicted[0]` to look at the model's assessment of the first sample*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeZvj7ekIhul"
   },
   "source": [
    "You'll notice that this took considerably longer to train than scikit-learn's logistic regression: this is because PyTorch is set-up to be more flexible and train architectures much more complex than a simple single neuron network. Scikit-learn's implementation of logistic regression is highly optimized.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* How many epochs would you need to increase the process to 200,000 iterations? ______________________\n",
    "* Does increasing to 200,000 iterations improve your test set accuracy? ______________________\n",
    "* Compare the predictions of your logistic regression from scikit-learn and your network developed with PyTorch. Are all the predictions the same? How many predictions are pair-wise different? ______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEscia6QIhul"
   },
   "source": [
    "Congratulations! You've completed an introduction to neural networks and PyTorch. If you want to explore more sophisicated architectures and applications, check out designing a PyTorch neural network to properly classify digit images here:\n",
    "\n",
    "https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
    "\n",
    "Other than that, you're done the lab!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
